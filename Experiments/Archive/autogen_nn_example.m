% Solve a Pattern Recognition Problem with a Neural Network
% Script generated by Neural Pattern Recognition app
% Created Tue Mar 24 22:02:54 EDT 2015
%
% This script assumes these variables are defined:
%
%   irisInputs - input data.
%   irisTargets - target data.

clear all;
load iris_dataset.mat


phi = mapminmax(irisInputs)';
% Normalize the data
%mean_sample = mean(phi);
%phi = phi - ones(size(phi, 1), 1)*mean_sample;
%phi_std = sqrt(var(phi));
%std_mat = ones(size(phi, 1), 1)*sqrt(var(phi));
%phi = phi ./ std_mat;

% Scale everything between -1 and 1
%phi = phi ./ max(max(abs(phi)));

t = irisTargets;

% Create a Pattern Recognition Network
hiddenLayerSize = 1;
net = patternnet(hiddenLayerSize);

% Choose Input and Output Pre/Post-Processing Functions
% For a list of all processing functions type: help nnprocess
net.input.processFcns = {};
net.output.processFcns = {};

%for iii = 1:numel(net.inputs{1}.processFcns)
%      phi = feval( net.inputs{1}.processFcns{iii}, ...
%          'apply', phi, net.inputs{1}.processSettings{iii} );
%end

%net.inputs{1}.processFcns = {'mapminmax'};
%net.outputs{2}.processFcns = {};


% Setup Division of Data for Training, Validation, Testing
% For a list of all data division functions type: help nndivide
net.divideFcn = 'dividerand';  % Divide data randomly
net.divideMode = 'sample';  % Divide up every sample
net.divideParam.trainRatio = 70/100;
net.divideParam.valRatio = 15/100;
net.divideParam.testRatio = 15/100;

% For help on training function 'trainscg' type: help trainscg
% For a list of all training functions type: help nntrain
net.trainFcn = 'trainscg';  % Scaled conjugate gradient

% Choose a Performance Function
% For a list of all performance functions type: help nnperformance
net.performFcn = 'mse';  % Cross-entropy

% Choose Plot Functions
% For a list of all plot functions type: help nnplot
net.plotFcns = {'plotperform','plottrainstate','ploterrhist', ...
  'plotregression', 'plotfit'};




% Train the Network
[net,tr] = train(net,phi',t);

%  Now that the network is trained, we can extract the weights and use
%  these in our stochastic network

% We can extract the biases for the (ith) layer with: net.b{i}  The result
% will be an (M) dimensional vector where M is the number of neurons in the
% (ith) layer.

% Find the normalization constant for all the weights / bias vectors
all_w = getwb(net);
%norm_c = max(abs(all_w));
norm_c = 1;

% Get the first layer inputs
l1_weights = [net.b{1}; net.IW{1}']./norm_c;

% Get the biases for the next layer which consists of three neurons
l2_biases = [net.b{2}]./norm_c;
l2_weights = [net.LW{2}]./norm_c;

% Get the weights of these three output neurons
l2wn1 = [l2_biases(1); l2_weights(1)]./norm_c;
l2wn2 = [l2_biases(2); l2_weights(2)]./norm_c;
l2wn3 = [l2_biases(3); l2_weights(3)]./norm_c;

% Lets loop through the inputs to see 
% TODO:  We have scaled the weights, but we have not adjusted the neuron
% output functions to correct for this change.  So, the outputs may not be
% correct.


N = 1024; %  Stochastic number length
FSM_STATES = 16;
AVG_STEPS = 1;

neuron_output = zeros(1,size(-1:.01:1,2));

for i = 1:AVG_STEPS
    
    results = [];
    for i = 1:size(phi,1)
        % Get the sample point
        x = (phi(i,:)');

        dec_signals = [1;x];
        
        % Convert the input to the Uni-Polar range
        dec_signals_squished = BIPOL_2_UNIPOL(dec_signals);
        
        % Convert the decimal values to stochastic numbers
        sc_signals = DEC2SC_ARRAY(dec_signals_squished, N);
        
        % Convert the decimal weights to stochastic weights
        %dec_weights should be a number between -1 and 1
        %dec_weights(1,:) = ones(1,size(dec_signals, 2))*0.7;
        dec_weights = ones(2,size(dec_signals, 2))*1;
        dec_weights_squished = BIPOL_2_UNIPOL(dec_weights);
        sc_weights = DEC2SC_ARRAY(dec_weights_squished, N);


        % Put this sample into the input layer.
        input_out = Neuron_Sigmoidal(dec_signals, l1_weights, 1);

        % Put the first output into the last layer
        out_1 = Neuron_Linear([1; input_out], l2wn1);
        out_2 = Neuron_Linear([1; input_out], l2wn2);
        out_3 = Neuron_Linear([1; input_out], l2wn3);

        % Use the softmax function to get a valid posterior distribution
        output = softmax([out_1; out_2; out_3])';

        output = output - max(output);
        output = (output >= 0);
        results = [results; output];
    end
    
% dec_signals should be a number between -1 and 1
%dec_signals(1,:) = -1:.01:1;
%dec_signals(2,:) = -1:.01:1;


sc_out = NEURON_PLAY(sc_signals, sc_weights, N);
dec_out = S2D_ARRAY(sc_out, N);
dec_out_expanded = UNIPOL_2_BIPOL(dec_out);

neuron_output = neuron_output + dec_out_expanded;
end

neuron_output = neuron_output ./ AVG_STEPS;
dec_out_correct = NEURON_TEST(dec_signals, dec_weights, FSM_STATES)

se_output = (neuron_output - dec_out_correct).^2;

figure % create new figure
subplot(2,1,1) % first subplot

plot(dec_signals, neuron_output);
legend('Neuron Output');

% Create xlabel
xlabel('Input','FontWeight','bold','FontSize',16);

% Create ylabel
ylabel('Output','FontWeight','bold','FontSize',16);

% Create title
title('Neuron Output','FontWeight','bold','FontSize',16);


subplot(2,1,2) % first subplot
plot(dec_signals, se_output);
legend('Output Error');

% Create xlabel
xlabel('Input','FontWeight','bold','FontSize',16);

% Create ylabel
ylabel('Squared Error','FontWeight','bold','FontSize',16);

% Create title
title('FSM Reset Error','FontWeight','bold','FontSize',16);

% Define network constants
FSM_STATES = 32;

results = [];
for i = 1:size(phi,1)
    % Get the sample point
    x = (phi(i,:)');
    
    % Put this sample into the input layer.
    input_out = SigmoidNeuron([1; x], l1_weights);
    
    % Put the first output into the last layer
    out_1 = LinearNeuron([1; input_out], l2wn1);
    out_2 = LinearNeuron([1; input_out], l2wn2);
    out_3 = LinearNeuron([1; input_out], l2wn3);
    
    % Use the softmax function to get a valid posterior distribution
    output = softmax([out_1; out_2; out_3])';
    
    output = output - max(output);
    output = (output >= 0);
    results = [results; output];
end

% Print the misclassification count error 
my_error = sum(sum(abs((results - t')), 2) > 0) / length(t);

% Test the network on all the data
output = sim(net, phi');
output = (output - ones(3, 1)*max(output)) >= 0;
net_error = sum(sum(abs((output' - t')), 2) > 0) / length(t);

% Test the Network
y = net(phi');
e = gsubtract(t,y);
tind = vec2ind(t);
yind = vec2ind(y);
percentErrors = sum(tind ~= yind)/numel(tind);
performance = perform(net,t,y)

% Recalculate Training, Validation and Test Performance
trainTargets = t .* tr.trainMask{1};
valTargets = t  .* tr.valMask{1};
testTargets = t  .* tr.testMask{1};
trainPerformance = perform(net,trainTargets,y)
valPerformance = perform(net,valTargets,y)
testPerformance = perform(net,testTargets,y)

% View the Network
view(net)

% Plots
% Uncomment these lines to enable various plots.
%figure, plotperform(tr)
%figure, plottrainstate(tr)
%figure, plotconfusion(t,y)
%figure, plotroc(t,y)
%figure, ploterrhist(e)

% Deployment
% Change the (false) values to (true) to enable the following code blocks.
if (false)
  % Generate MATLAB function for neural network for application deployment
  % in MATLAB scripts or with MATLAB Compiler and Builder tools, or simply
  % to examine the calculations your trained neural network performs.
  genFunction(net,'myNeuralNetworkFunction');
  y = myNeuralNetworkFunction(x);
end
if (false)
  % Generate a matrix-only MATLAB function for neural network code
  % generation with MATLAB Coder tools.
  genFunction(net,'myNeuralNetworkFunction','MatrixOnly','yes');
  y = myNeuralNetworkFunction(x);
end
if (false)
  % Generate a Simulink diagram for simulation or deployment with.
  % Simulink Coder tools.
  gensim(net);
end
